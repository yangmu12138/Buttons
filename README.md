# Buttons
How LLM works--

For some degree ,the LLM can run efficiently more due to reason that it loaded all the effective language message from the whole planet , as we got Embbedding skill , after trainning , all the embed tokens will be arranged sparsely or densely corresbonded to the links between each point in the vector space . The prosess of Back propogation will finally get the probability about the liquidity between different vectors or vector clusters , and binding them with weighing(w) and bias(b). Although these weighing and bias which recording liquidities between different vectors just linked with finite units , but for that the vectors had linked with other units through Embedding progress , so actually we can't make sure that whether there's no relationship between two vectors even their distance is far enough . And this leads to two ends : first, we must loaded the whole model and related parameters when we apply it ; second ,once there's more language message created ,and these new tokens didn't get enough similarities with the ones in Embedding space,(as to say we can't solve it through Fine tuning)then we need to mix these new and old message,and train the model again .

LLM can real be logical ?--

Any problems occur just because there's enough unknown factors ,once the message space that surround the problem finded , then problems didn't exit .And the ability that to link these masssages or to expand the message space ,we call it ligic.To solve different problems equal to activate different message space. Of course ,these linked spaces maybe be paralleled ,or behaved like recursion structures.Because language is more like the descriptions about the World model,surely include the descriptions about how human solve real problems ,hence we can believe LLM has given the logical capability indirect.But still ,we must recognize that the ability to carry the real world's message about the language is limited,so we can't expect the LLM can really hold strong ligical capability. Human language more like a instruction book,although we try to make it more detailed ,but still there's big hell far from the real world's description.

Why LLM can't build real agi and how we make it ?--

We've know that actually Human language lacks the ability to represent more details of the real world, besides ,although most LLM be trained through Multi-mode skill nowadays ,but still,it more like to translate the language message to other modes.That's to say,the base of LLM's message still based on language.So if we really wanna create the real AGI,and not only create it as a Language expert,we must train it based on more messages that can more represent the real world.Actually,we can display all kinds of message in visual mode.First,most of the world's message exit in visual style,and no matter words,voice or vibrate...we can all transpose them in visual mode.
Of course,there's another problem,in most exited models,we've use Embedding skill to construct the vector space that record the links between different units,this make that when we input some tokens into it,the model can quickly activate the area that got links with the inputs.But as a more kinds of complicated message,and we expect the model can continually growing,so we can't build some kinds of message space like Embedding.Now,we need new structures.And if possible,we don't like it load the whole world model and parameters when we run it.As it happens,it'll reduce more cost.And based on it,Robots can real get the possibility that cost less than Human.


