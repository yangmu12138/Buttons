# Buttons
How LLM works--

In some degree ,the LLM can run efficiently more due to the reason that it loaded all the effective language message from the whole planet , as we got Embbedding skill , after trainning , all the embed tokens will be arranged sparsely or densely corresbonded to the links between each point in the vector space . The prosess of Back propogation will finally get the probability about the liquidity between different vectors or vector clusters , and binding them with weighing(w) and bias(b). Although these weighing and bias which recording liquidities between different vectors just linked with finite units , but for that the vectors had linked with other units through Embedding progress , so actually we can't make sure that whether there's no relationship between two vectors even their distance is far enough . And this leads to two ends : first, we must loaded the whole model and related parameters when we apply it ; second ,once there's more language message created ,and these new tokens didn't get enough similarities with the ones in Embedding space,(as to say we can't solve it through Fine tuning)then we need to mix these new and old message,and train the model again .

LLM can real be logical ?--

Any problems occur just because there's enough unknown factors ,once the message space that surround the problem finded , then problems didn't exit .And the ability that to link these masssages or to expand the message space ,we call it ligic.To solve different problems equal to activate different message space. Of course ,these linked spaces maybe be paralleled ,or behaved like recursion structures.Because language is more like the descriptions about the World model,surely include the descriptions about how human solve real problems ,hence we can believe LLM has given the logical capability indirect.But still ,we must recognize that the ability to carry the real world's message about the language is limited,so we can't expect the LLM can really hold strong ligical capability. Human language more like a instruction book,although we try to make it more detailed ,but still there's big hell far from the real world's description.

Why LLM can't build real agi and how we make it ?--

We've know that actually Human language lacks the ability to represent more details of the real world, besides ,although most LLM be trained through Multi-mode skill nowadays ,but still,it more like to translate the language message to other modes.That's to say,the base of LLM's message still based on Human language.So if we really wanna create the real AGI,and not only create it as a Language expert,we must train it based on more messages that can more represent the real world.Actually,we can display all kinds of message in visual mode.First,most of the world's message exit in visual style,and no matter words,voice or vibrate...we can all transpose them in visual mode.
Of course,there's another problem,in most exited models,we've use Embedding skill to construct the vector space that record the links between different units,this make that when we input some tokens into it,the model can quickly activate the area that got links with the inputs.But as a more kinds of complicated message,and we expect the model can continually growing,so we can't build some kinds of message space like Embedding.Now,we need new structures.And if possible,we don't like it load the whole world model and parameters when we run it.As it happens,it'll reduce more cost.And based on it,Robots can real get the possibility that cost less than Human.

![IMG20250620203627](https://github.com/user-attachments/assets/6d01dc9b-2467-4169-a00b-a25aea21479e)

We need new structure--

Spiking neuron Network? I've explore it for a long time,include MIT's Liquid Time-constant Network.Most people try to use it edge out Transformer or other rnn Model, just because it seems that these units maybe can make Constant-Learning case comes true.But there's a pity that the ability of it about growing didn't last too long,or maybe we can say the sensory density of these units is limited.
But think about another case ,how about just make it as a activator?Train it base on visual message,let all visual tokens flowing through these units before trained by other Rnn's like models.And here i need to supplement that we can also let the visualed Language message flowing through it,it'll make it easier to be activated while we apply the model.In this process,these activators will be trained sensitive enough for the related message.Imagine we got massive units,then we put them in one big box.When we run the box,no matter the inputs in visual mode or just other visualed kinds of message,the related activator will be activated,and then,the linked model and its trained parameters will be loaded,this helps the massage space be built.

How to store the trained model and parameters--


